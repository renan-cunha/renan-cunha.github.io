<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Since contextual bandits are relatively new, they haven&rsquo;t been studied and tested so heavily as standard multi-armed bandits algorithms [1]. This being the case, it&rsquo;s useful to design benchmarks for contextual bandits so these algorithms can be reliably tested, compared and improved.
Prerequisites A Very Short Intro to Contextual Bandits Simplest benchmark Many people in the AI community recommend the use of toy problems, these low complexity examples provide better and faster debugging cycles, a good property of early-stage research and experimentation.">  

  <title>
    
      Brainstorming Benchmarks for Contextual Bandits
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
  
  
  
  <link rel="stylesheet" href="/css/main.e6c6ad7f6873a1e65e553732d1474862d648641ca12a447a3090064ed82f37acd184ac97902a20c4adb079ea8e51f77cac1cb3b0d62818b77fc0bc54148863a2.css" integrity="sha512-5satf2hzoeZeVTcy0UdIYtZIZByhKkR6MJAGTtgvN6zRhKyXkCogxK2weeqOUfd8rByzsNYoGLd/wLxUFIhjog==" />
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<body a="dark">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2020-02-25 00:00:00 &#43;0000 UTC">
            2020-02-25
        </time>
    </p>

    <h1>Brainstorming Benchmarks for Contextual Bandits</h1>

    

    <p>Since contextual bandits are relatively new, they haven&rsquo;t been studied and tested
so heavily as standard multi-armed bandits algorithms [1]. This being
the case, it&rsquo;s useful to design benchmarks for contextual bandits so these
algorithms can be reliably tested, compared and improved.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li><a href="/posts/contextual-bandits/"><strong><em>A Very Short Intro to Contextual Bandits</em></strong></a></li>
</ul>
<h2 id="simplest-benchmark">Simplest benchmark</h2>
<p><a href="https://twitter.com/pcastr/status/1226661835971145731"><strong><em>Many</em></strong></a>
<a href="https://twitter.com/karpathy/status/1013244313327681536"><strong><em>people</em></strong></a> in the
AI community <a href="https://rockt.github.io/2018/08/29/msc-advice"><strong><em>recommend</em></strong></a>
the use of toy problems, these low complexity examples
provide better and faster
<a href="https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html"><strong><em>debugging cycles</em></strong></a>,
a good property of early-stage research and experimentation. So now, let&rsquo;s try to imagine the simplest benchmark for the contextual bandits
problem. First, let&rsquo;s
use binary everywhere.
Second, let&rsquo;s make the environment deterministic (an action <em>k</em> with context <em>x</em>
always returns the same reward <em>r</em>). Third and last, let&rsquo;s make
the reward function <em>f(x, k</em>) as simple as possible. Remember that this reward
function is calculated given the observed context and the chosen action.</p>
<p>So, this is what I came about:</p>
<ul>
<li>Context: 1 binary feature</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>if context == 0
</span></span><span style="display:flex;"><span>    action 0 gives a reward of 1, other actions give 0 
</span></span><span style="display:flex;"><span>    rewards
</span></span><span style="display:flex;"><span>else
</span></span><span style="display:flex;"><span>    action 1 gives a reward of 1, other actions give 0 
</span></span><span style="display:flex;"><span>    rewards
</span></span></code></pre></div><p>In this case, the best policy is to choose the $0^{th}$ action if the context is 0
and choose the $1^{st}$ action otherwise, easy.</p>
<h2 id="how-to-complicate">How to complicate</h2>
<p>Now, let&rsquo;s try to gradually add complexity to this problem.</p>
<h3 id="stochastic-rewards">Stochastic rewards</h3>
<p>Stochasticity always makes things harder and since this is what we are looking
for, let&rsquo;s add it to our environment.</p>
<ul>
<li>Context: 1 binary feature</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>if context == 0
</span></span><span style="display:flex;"><span>    action 0 returns a reward of 2 with 50% chance, other 
</span></span><span style="display:flex;"><span>    actions give 0 rewards
</span></span><span style="display:flex;"><span>else
</span></span><span style="display:flex;"><span>    action 1 returns a reward of 2 with 50% chance, other 
</span></span><span style="display:flex;"><span>    actions give 0 rewards
</span></span></code></pre></div><p>Here, the best policy is still the same as in the previous environment, but
now, it should be harder for an algorithm to discover it. In this
example, the reward is 2 because of the stochasticity, so the expected reward
of each context can still be 1.</p>
<h3 id="continuous-context">Continuous context</h3>
<p>Instead of the context being made of only 1 binary feature, it makes sense to use
continuous ones, in case of the age of someone, for example, is being represented
as a feature. So, let&rsquo;s change
the context possibilities to infinity!</p>
<ul>
<li>Context: 1 continuous feature on the interval [0, 1]</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>if context &gt; 0.5
</span></span><span style="display:flex;"><span>    action 0 gives a reward of 2 with 50% chance, other 
</span></span><span style="display:flex;"><span>    actions give 0 rewards
</span></span><span style="display:flex;"><span>else
</span></span><span style="display:flex;"><span>    action 1 gives a reward of 2 with 50% chance, other 
</span></span><span style="display:flex;"><span>    actions give 0 rewards
</span></span></code></pre></div><p>Note that we have to change the first <code>if</code> case of our reward function to
deal with the continuous feature. Instead of testing equality, we test if the
context is greater than a threshold, in this case, 0.5.</p>
<h3 id="other-ways">Other ways</h3>
<p>There are other ways to increase the complexity of the problem and
I will list some below.</p>
<h3 id="datasets">Datasets</h3>
<p>While it is good to manually tweak a contextual bandit environment for research
or some experiments,
this is not a straight forward task and requires dedicated human thought. A
good alternative is to test contextual
bandit algorithms on supervised-learning datasets. This can be done
by representing the context and action by each (<em>X, y</em>) feature vector pair
of the dataset, both regression
and classification datasets can be used. With this approach, the environment can randomly sample
one instance of the dataset, and the reward is based on the $y^k$ raw value in
the case
of regression. With classification datasets, we can use accuracy as the reward.</p>
<h2 id="some-possibilities-for-benchmarks">Some possibilities for benchmarks</h2>
<p>Here, I try to imagine more benchmarks for this problem, trying to add complexity
and deal with corner cases.</p>
<ul>
<li>Standard multi-armed bandits scenario: the context doesn&rsquo;t influence the
reward function.</li>
<li>Imbalanced Dataset: There is an action that is better more often.</li>
<li>Type of reward function function instead of just <code>context &gt; 0.5</code>:
<ul>
<li>linear function</li>
<li>non-linear function (neural network)</li>
</ul>
</li>
<li>Many degrees of randomness: This can be made by artificially adding noise to
the dataset.</li>
<li>Vary the number of actions and the number of context features: It would be good
to test algorithms on datasets that have a high and low number of actions, this can
also be said about the dimensionality of the context.</li>
<li>Vary the number of samples: If the context repeats more often, then it
should be easier for the algorithm to learn.</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>[1] Cortes, David. “Adapting Multi-Armed Bandits Policies to Contextual Bandits Scenarios.” ArXiv:1811.04383 [Cs, Stat], Nov. 2019. arXiv.org, <a href="http://arxiv.org/abs/1811.04383">http://arxiv.org/abs/1811.04383</a>.</li>
<li>[2] Bietti, Alberto, et al. “A Contextual Bandit Bake-Off.” ArXiv:1802.04064 [Cs, Stat], Jan. 2020. arXiv.org, <a href="http://arxiv.org/abs/1802.04064">http://arxiv.org/abs/1802.04064</a>.</li>
</ul>

</article>

            </div>
        </main>
    </body></html>
