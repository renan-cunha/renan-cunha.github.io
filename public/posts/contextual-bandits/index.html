<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="In the contextual bandits problem, we have an environment with K possible actions. The environment returns some context x, taken from a distribution X. The environment also has a function f(x, k), that calculates the reward based on the context and the chosen action k.
The goal then is to find the policy $\pi$ that maximizes the rewards obtained in the long-term. A policy is a function that maps contexts to actions.">  

  <title>
    
      A Very Short Intro to Contextual Bandits
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
  
  
  
  <link rel="stylesheet" href="/css/main.e6c6ad7f6873a1e65e553732d1474862d648641ca12a447a3090064ed82f37acd184ac97902a20c4adb079ea8e51f77cac1cb3b0d62818b77fc0bc54148863a2.css" integrity="sha512-5satf2hzoeZeVTcy0UdIYtZIZByhKkR6MJAGTtgvN6zRhKyXkCogxK2weeqOUfd8rByzsNYoGLd/wLxUFIhjog==" />
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<body a="dark">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2020-02-24 00:00:00 &#43;0000 UTC">
            2020-02-24
        </time>
    </p>

    <h1>A Very Short Intro to Contextual Bandits</h1>

    

    <p>In the contextual bandits problem, we have an
environment with <em>K</em> possible actions. The environment returns some context <em>x</em>, taken from a distribution <em>X</em>.
The environment also has a function <em>f(x, k)</em>, that calculates the reward
based on the context and the chosen action <em>k</em>.</p>
<p>The goal then is to find the policy $\pi$ that maximizes the rewards obtained in the
long-term. A policy is a function that maps contexts to actions.</p>
<p>The diagram below illustrates the problem:</p>
<p><img src="/intro.png" alt="contextual-bandits-diagram"></p>
<p>Explaining the steps:</p>
<ul>
<li>The environment displays a context <em>x</em>.</li>
<li>The agent chooses an action <em>k</em> based on the observed context.</li>
<li>The environment returns a reward based on the most recent context and action.</li>
</ul>
<p>This process continues without a determined time-limit.</p>
<p>Additional considerations can be taken into account, like statistical efficiency (if the
algorithm learns fast considering the number of examples) and
computational complexity.</p>
<h2 id="example">Example</h2>
<ul>
<li>Agent: A doctor.</li>
<li>Environment: Every day, a random patient comes into the hospital with a
disease X (the disease is the same to all patients).
<ul>
<li>Context: Features of the patient like age, sex, etc.</li>
<li>Actions: K-number of medicines that the doctor can prescribe.</li>
<li>Reward: 1 if the patient was cured, 0 otherwise.</li>
</ul>
</li>
</ul>
<h2 id="references">References</h2>
<p>Slivkins, Aleksandrs. “Introduction to Multi-Armed Bandits.” ArXiv:1904.07272 [Cs, Stat], Sept. 2019. arXiv.org, <a href="http://arxiv.org/abs/1904.07272">http://arxiv.org/abs/1904.07272</a>.</p>

</article>

            </div>
        </main>
    </body></html>
