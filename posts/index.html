<!DOCTYPE html>
<html >
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="generator" content="Hugo 0.74.3" />
	
	<title>Posts - </title>

	
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
	<link href="https://renan-cunha.github.io//css/bootstrap.min.css" rel="stylesheet">
	<link href="https://renan-cunha.github.io//css/strange-case.css" rel="stylesheet">
	

	
	
	<!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
	
    <link href="https://renan-cunha.github.io/posts/index.xml" rel="alternate" type="application/rss+xml" title="Renan Cunha" />
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

</head>
<body class="scheme-lightbrown">


	<div class="container-fluid">
		<div class="row">
			<div class="col-sm-4 col-md-3 col-lg-3 sidebar">

			
	<div class="sidebar-content">

		<a href="https://renan-cunha.github.io/"><h1>Renan Cunha</h1></a>
		<p></p>

		
		<div class="sidebar-freetext">
			<p>Computer Science student with interest in AI</p>
		</div>
		

		<ul class="sidebar-menus">
			
				<li>&#187; <a href="https://renan-cunha.github.io/about/">About Me</a> </li>
			
				<li>&#187; <a href="https://renan-cunha.github.io/archive/">Archive</a> </li>
			
				<li>&#187; <a href="https://renan-cunha.github.io/posts">Blog</a> </li>
			
				<li>&#187; <a href="https://renan-cunha.github.io/categories">Categories</a> </li>
			
		</ul>

		<div class="sidebar-recent hidden-xs">
			<p>Recent Posts:</p>
			<ul>
				
				<li><a href="https://renan-cunha.github.io/categories/ai/">AI</a></li>
				
				<li><a href="https://renan-cunha.github.io/categories/">Categories</a></li>
				
				<li><a href="https://renan-cunha.github.io/categories/contextual-bandits/">Contextual Bandits</a></li>
				
				<li><a href="https://renan-cunha.github.io/posts/">Posts</a></li>
				
				<li><a href="https://renan-cunha.github.io/categories/reinforcement-learning/">Reinforcement Learning</a></li>
				
			</ul>
		</div>

		

		<p class="copyright">&copy; 2020. All rights reserved. </p>
		<p class="attr">Powered by <a href="http://gohugo.io">Hugo</a> &amp; <a href="https://github.com/ExchangeRate-API/strange-case">Strange Case</a> (inspired by <a href="https://github.com/poole/hyde">Hyde</a>).</p>

	</div>


			</div>
			<div class="col-sm-7 col-sm-offset-4 col-md-6 col-md-offset-3 col-lg-5 col-lg-offset-3 content">

				
				<div class="post">

					<div class="post-heading">
						<h1><a href="https://renan-cunha.github.io/posts/greedy/">Solving Contextual Bandits with Greediness</a></h1>
						<span>Mar 2, 2020</span>
					</div>

					<p>In this post, I will explain and implement Epsilon-Greedy, a simple algorithm that solves
the contextual bandits problem. Despite its
simplicity, this algorithm performs
considerably well [1].</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://renan-cunha.github.io/posts/contextual-bandits/"><strong><em>A Very Short Intro to Contextual Bandits</em></strong></a></li>
<li>Python</li>
<li><a href="https://numpy.org/devdocs/user/quickstart.html"><strong><em>Numpy</em></strong></a></li>
<li>(Optional) Standard Multi-Armed Bandit Epsilon-Greedy Algorithm [2]</li>
<li><a href="https://www.youtube.com/watch?v=-la3q9d7AKQ&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;index=33&amp;t=0s"><strong><em>Logistic Regression</em></strong></a> (You need
to know what it is, not necessarily how it works)</li>
</ul>
<p>* Note: In this article, I use the words <em>arm</em> and <em>action</em>, and the words
<em>step</em> and <em>round</em>, interchangeably.</p>
<h2 id="intro">Intro</h2>
<p>The intuition of the algorithm is to choose random actions at sometimes. In the
rest of the time, it chooses the action that it thinks is the best one. This
simple balance between exploration (random actions) and exploitation (greediness)
yields good results.</p>
<h2 id="pseudocode">Pseudocode</h2>
<p><img src="https://renan-cunha.github.io/img/epsilon-greedy-pseudocode.png" alt="epsilon-greedy-pseudocode"></p>
<p>This pseudocode is from [3].</p>
<h3 id="input">Input</h3>
<p>We can see that the algorithm receives three arguments, a probability <em>p</em>, a decay
rate <em>d</em> and the oracles $\hat{f}_{1: k}$. Let&rsquo;s explain each one of them.</p>
<ul>
<li><em>p</em>: Dictates what will be the probability of a random action in each round.</li>
<li><em>d</em>: This variable controls how fast <em>p</em> decreases during training.</li>
<li>$\hat{f}_{1: k}$: Oracles that are classifiers or regressors. They learn the
rewards that each action will return, given the context <em>x</em>.</li>
</ul>
<h3 id="line-by-line-analysis">Line-by-Line Analysis</h3>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_line_1.png" alt="epsilon-greedy-line-1"></p>
<p>This is the standard loop of contextual bandits, in which the algorithm
receives a context <em>x</em> in each round.</p>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_line_2.png" alt="epsilon-greedy-line-2"></p>
<p>In this block, the algorithm chooses a random action with probability <em>p</em>.
Otherwise, it chooses the action that gives the maximum reward according to the
oracles.</p>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_line_6.png" alt="epsilon-greedy-line-6"></p>
<p>The probability rate is decreased according to <em>d</em>.</p>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_line_7.png" alt="epsilon-greedy-line-7"></p>
<p>The algorithm receives a reward $r_a^t$ and stores it together with the
observed context $x^t$. Note that the data stored is exclusive to the performed
action/arm <em>a</em>.</p>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_line_8.png" alt="epsilon-greedy-line-8"></p>
<p>Then, the oracles learn the reward of each (context, action) pair given the
data history. This training does not need to happen in every round. It is possible, for
example, to train the oracles every 50 rounds.</p>
<h2 id="code">Code</h2>
<p>The programming language used in this implementation is Python and the full
implementation
is available <a href="https://colab.research.google.com/drive/1OemCb50Suc6oklrmmsgORBIqHBIaPD8V"><strong><em>here</em></strong></a>. For simplicity, the decay rate <em>d</em> will be
discarded and it will be assumed that the probability of a random action will
remain constant. Only binary rewards will be used. For the oracles, logistic regressors of the
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"><strong><em>Sklearn library</em></strong></a> have been chosen.</p>
<h3 id="contextual-environment">Contextual Environment</h3>
<p>I made an interface of the contextual bandits environment so that I can easily
request contexts and get rewards by acting on the environment.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ContextualEnv</span>(ABC):
    
    <span style="color:#a6e22e">@abstractmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_context</span>(self) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
        <span style="color:#66d9ef">pass</span>

    <span style="color:#a6e22e">@abstractmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">act</span>(self, action: int) <span style="color:#f92672">-&gt;</span> float:
        <span style="color:#66d9ef">pass</span>

    <span style="color:#a6e22e">@abstractmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_num_arms</span>(self) <span style="color:#f92672">-&gt;</span> int:
        <span style="color:#66d9ef">pass</span>

    <span style="color:#a6e22e">@abstractmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_context_dim</span>(self) <span style="color:#f92672">-&gt;</span> int:
        <span style="color:#66d9ef">pass</span>
</code></pre></div><h3 id="epsilon-greedy-constructor">Epsilon Greedy Constructor</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EpsilonGreedy</span>:

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, c_env: ContextualEnv, 
                 epsilon: float, num_steps: int,
                 training_freq: int):
        self<span style="color:#f92672">.</span>num_arms <span style="color:#f92672">=</span> c_env<span style="color:#f92672">.</span>get_num_arms()
        self<span style="color:#f92672">.</span>num_steps <span style="color:#f92672">=</span> num_steps
        self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> epsilon
        self<span style="color:#f92672">.</span>training_freq <span style="color:#f92672">=</span> training_freq
        self<span style="color:#f92672">.</span>classifiers <span style="color:#f92672">=</span> [LogisticRegression() <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> 
                            range(self<span style="color:#f92672">.</span>num_arms)]
        context_dim <span style="color:#f92672">=</span> c_env<span style="color:#f92672">.</span>get_context_dim()
        self<span style="color:#f92672">.</span>context_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((num_steps,
                                      context_dim))
        self<span style="color:#f92672">.</span>rewards_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>full((num_steps, 
                                     self<span style="color:#f92672">.</span>num_arms), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, 
                                    dtype<span style="color:#f92672">=</span>float)
        self<span style="color:#f92672">.</span>c_env <span style="color:#f92672">=</span> c_env
</code></pre></div><p>Here, epsilon is the probability of a random action <em>p</em> of the pseudocode.
This class also receives a contextual environment object, the number of
steps/rounds, and a parameter that dictates the training frequency of
the oracles.</p>
<p>One logistic regressor oracle is initialized for
each arm/action of the environment. The context and reward history are also
initialized. Since binary rewards are being used, the reward history is filled with
-1, so it is possible to differentiate what arm received a reward in each round
(a value of -1 says that the respective arm was not used in a given round).</p>
<h3 id="save-stepround">Save Step/Round</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_step</span>(self, context: np<span style="color:#f92672">.</span>ndarray, action: int,
              reward: float, step: int) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
    self<span style="color:#f92672">.</span>context_data[step] <span style="color:#f92672">=</span> context
    self<span style="color:#f92672">.</span>rewards_data[step, action] <span style="color:#f92672">=</span> reward
</code></pre></div><p>This method saves the context and reward received in each round/step. Note
that the reward is saved on the history of the action/arm used.</p>
<h3 id="action-policy">Action Policy</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">action_policy</span>(self, context: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> int:
    coin <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">if</span> coin <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>epsilon:
        action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>greedy_action(context)
    <span style="color:#66d9ef">else</span>:
        action <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>num_arms<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> action
</code></pre></div><p>This method determines which strategy will be chosen, exploration or exploitation.
The random action is chosen with a probability equal to epsilon, the greedy
action is chosen otherwise.</p>
<h3 id="greedy-action">Greedy Action</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">greedy_action</span>(self, context: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> int:
    rewards <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(self<span style="color:#f92672">.</span>classifiers))
    <span style="color:#66d9ef">for</span> index, classifier <span style="color:#f92672">in</span> enumerate(self<span style="color:#f92672">.</span>classifiers):
        <span style="color:#66d9ef">try</span>:
            context <span style="color:#f92672">=</span> context<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
            action_score <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(context)
        <span style="color:#66d9ef">except</span> NotFittedError <span style="color:#66d9ef">as</span> e:
            a <span style="color:#f92672">=</span> <span style="color:#ae81ff">3.0</span><span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>num_arms
            action_score <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>beta(a, <span style="color:#ae81ff">4</span>)
        rewards[index] <span style="color:#f92672">=</span> action_score

     max_rewards <span style="color:#f92672">=</span> max(rewards)
     best_actions <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(rewards <span style="color:#f92672">==</span> max_rewards)
     best_actions <span style="color:#f92672">=</span> best_actions<span style="color:#f92672">.</span>flatten()
     <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(best_actions)
</code></pre></div><p>On the greedy action method, each classifier is evaluated based on the context.
If the classifier has not yet been trained, the score is estimated by running a beta
distribution. This trick is done on [3]. Afterwards, the action
with the maximum estimated reward is chosen.</p>
<h3 id="fit">Fit</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, step: int) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
    step <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    contexts_so_far <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>context_data[:step]
    rewards_so_far <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>rewards_data[:step]
    <span style="color:#66d9ef">for</span> arm_index <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_arms):
        self<span style="color:#f92672">.</span>fit_classifier(contexts_so_far,
                            rewards_so_far, arm_index)
</code></pre></div><p>In this method, each classifier is trained, only the contexts and rewards
seen so far are used.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit_classifier</span>(self, contexts: np<span style="color:#f92672">.</span>ndarray,
                   rewards: np<span style="color:#f92672">.</span>ndarray,
                   arm: int) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
    arm_rewards <span style="color:#f92672">=</span> rewards[:, arm]
    <span style="color:#75715e"># get the index of the rewards that the arm saw</span>
    index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(arm_rewards <span style="color:#f92672">!=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    index <span style="color:#f92672">=</span> index<span style="color:#f92672">.</span>flatten()
    arm_rewards <span style="color:#f92672">=</span> arm_rewards[index]
    <span style="color:#75715e"># test if the arm saw at least one example of </span>
    <span style="color:#75715e"># each class</span>
    <span style="color:#66d9ef">if</span> len(np<span style="color:#f92672">.</span>unique(arm_rewards)) <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
        arm_contexts <span style="color:#f92672">=</span> contexts[index]
        self<span style="color:#f92672">.</span>classifiers[arm]<span style="color:#f92672">.</span>fit(arm_contexts, 
                                  arm_rewards)
</code></pre></div><p>Here, only the rewards that the each arm saw are used (that is why the
reward history was initialized with -1, note the <code>argwhere</code>). It is verified if the arm saw at least one example
of each reward (values of 0 and 1), so it can be trained.</p>
<h3 id="simulation">Simulation</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simulate</span>(self) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Returns rewards per step&#34;&#34;&#34;</span>

    rewards_history <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(self<span style="color:#f92672">.</span>num_steps)
    <span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_steps):
        context <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_env<span style="color:#f92672">.</span>get_context()
        action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>action_policy(context)
        reward <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_env<span style="color:#f92672">.</span>act(action)
        rewards_history[step] <span style="color:#f92672">=</span> reward
        self<span style="color:#f92672">.</span>save_step(context, action, reward, step)
        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>training_freq <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            self<span style="color:#f92672">.</span>fit(step)
        
    <span style="color:#66d9ef">return</span> rewards_history
</code></pre></div><p>The <code>simulate</code> method does the training itself. A context is observed, an action
is estimated based on the context, the (context, reward) is stored on the
respective arm history, and every <code>training_freq</code> steps, the oracles are trained.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Let&rsquo;s evaluate the algorithm on three environments, taken from
<a href="https://renan-cunha.github.io/posts/benchmark/"><strong><em>here</em></strong></a>. I will give a brief
description of each one:</p>
<ol>
<li>Binary context and deterministic rewards (easiest).</li>
<li>Binary context and stochastic rewards.</li>
<li>Continuous context and stochastic rewards.</li>
</ol>
<p>The results were obtained using epsilon equal to 0.2, training the oracles every
50 rounds and averaging the results of 100 runs. This plot considers the mean
reward of all history until each round.</p>
<p><img src="https://renan-cunha.github.io/img/epsilon_greedy_results.png" alt="epsilon-greedy-results"></p>
<p>We can see that the algorithm learns the deterministic binary environment really fast.
The performance on the stochastic binary and stochastic continuous environments
are about the same, but the algorithm has a little more trouble to learn the
continuous environment.</p>
<h2 id="references">References</h2>
<ul>
<li>[1] Bietti, Alberto, et al. “A Contextual Bandit Bake-Off.” ArXiv:1802.04064 [Cs, Stat], Jan. 2020. arXiv.org, <a href="http://arxiv.org/abs/1802.04064">http://arxiv.org/abs/1802.04064</a>.</li>
<li>[2] Chapter 2 of Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a></li>
<li>[3] Cortes, David. “Adapting Multi-Armed Bandits Policies to Contextual Bandits Scenarios.” ArXiv:1811.04383 [Cs, Stat], Nov. 2019. arXiv.org, <a href="http://arxiv.org/abs/1811.04383">http://arxiv.org/abs/1811.04383</a>.</li>
</ul>
<p>Previous Article: <a href="https://renan-cunha.github.io/posts/benchmark/"><strong><em>Brainstorming Benchmarks for Contextual Bandits</em></strong></a></p>


				</div>
				
				<div class="post">

					<div class="post-heading">
						<h1><a href="https://renan-cunha.github.io/posts/benchmark/">Brainstorming Benchmarks for Contextual Bandits</a></h1>
						<span>Feb 25, 2020</span>
					</div>

					<p>Since contextual bandits are relatively new, they haven&rsquo;t been studied and tested
so heavily as standard multi-armed bandits algorithms [1]. This being
the case, it&rsquo;s useful to design benchmarks for contextual bandits so these
algorithms can be reliably tested, compared and improved.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://renan-cunha.github.io/posts/contextual-bandits/"><strong><em>A Very Short Intro to Contextual Bandits</em></strong></a></li>
</ul>
<h2 id="simplest-benchmark">Simplest benchmark</h2>
<p><a href="https://twitter.com/pcastr/status/1226661835971145731"><strong><em>Many</em></strong></a>
<a href="https://twitter.com/karpathy/status/1013244313327681536"><strong><em>people</em></strong></a> in the
AI community <a href="https://rockt.github.io/2018/08/29/msc-advice"><strong><em>recommend</em></strong></a>
the use of toy problems, these low complexity examples
provide better and faster
<a href="https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html"><strong><em>debugging cycles</em></strong></a>,
a good property of early-stage research and experimentation. So now, let&rsquo;s try to imagine the simplest benchmark for the contextual bandits
problem. First, let&rsquo;s
use binary everywhere.
Second, let&rsquo;s make the environment deterministic (an action <em>k</em> with context <em>x</em>
always returns the same reward <em>r</em>). Third and last, let&rsquo;s make
the reward function <em>f(x, k</em>) as simple as possible. Remember that this reward
function is calculated given the observed context and the chosen action.</p>
<p>So, this is what I came about:</p>
<ul>
<li>Context: 1 binary feature</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<pre><code>if context == 0
    action 0 gives a reward of 1, other actions give 0 
    rewards
else
    action 1 gives a reward of 1, other actions give 0 
    rewards
</code></pre><p>In this case, the best policy is to choose the $0^{th}$ action if the context is 0
and choose the $1^{st}$ action otherwise, easy.</p>
<h2 id="how-to-complicate">How to complicate</h2>
<p>Now, let&rsquo;s try to gradually add complexity to this problem.</p>
<h3 id="stochastic-rewards">Stochastic rewards</h3>
<p>Stochasticity always makes things harder and since this is what we are looking
for, let&rsquo;s add it to our environment.</p>
<ul>
<li>Context: 1 binary feature</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<pre><code>if context == 0
    action 0 returns a reward of 2 with 50% chance, other 
    actions give 0 rewards
else
    action 1 returns a reward of 2 with 50% chance, other 
    actions give 0 rewards
</code></pre><p>Here, the best policy is still the same as in the previous environment, but
now, it should be harder for an algorithm to discover it. In this
example, the reward is 2 because of the stochasticity, so the expected reward
of each context can still be 1.</p>
<h3 id="continuous-context">Continuous context</h3>
<p>Instead of the context being made of only 1 binary feature, it makes sense to use
continuous ones, in case of the age of someone, for example, is being represented
as a feature. So, let&rsquo;s change
the context possibilities to infinity!</p>
<ul>
<li>Context: 1 continuous feature on the interval [0, 1]</li>
<li>Number of actions: 2</li>
<li>Reward function pseudocode:</li>
</ul>
<pre><code>if context &gt; 0.5
    action 0 gives a reward of 2 with 50% chance, other 
    actions give 0 rewards
else
    action 1 gives a reward of 2 with 50% chance, other 
    actions give 0 rewards
</code></pre><p>Note that we have to change the first <code>if</code> case of our reward function to
deal with the continuous feature. Instead of testing equality, we test if the
context is greater than a threshold, in this case, 0.5.</p>
<h3 id="other-ways">Other ways</h3>
<p>There are other ways to increase the complexity of the problem and
I will list some below.</p>
<h3 id="datasets">Datasets</h3>
<p>While it is good to manually tweak a contextual bandit environment for research
or some experiments,
this is not a straight forward task and requires dedicated human thought. A
good alternative is to test contextual
bandit algorithms on supervised-learning datasets. This can be done
by representing the context and action by each (<em>X, y</em>) feature vector pair
of the dataset, both regression
and classification datasets can be used. With this approach, the environment can randomly sample
one instance of the dataset, and the reward is based on the $y^k$ raw value in
the case
of regression. With classification datasets, we can use accuracy as the reward.</p>
<h2 id="some-possibilities-for-benchmarks">Some possibilities for benchmarks</h2>
<p>Here, I try to imagine more benchmarks for this problem, trying to add complexity
and deal with corner cases.</p>
<ul>
<li>Standard multi-armed bandits scenario: the context doesn&rsquo;t influence the
reward function.</li>
<li>Imbalanced Dataset: There is an action that is better more often.</li>
<li>Type of reward function function instead of just <code>context &gt; 0.5</code>:
<ul>
<li>linear function</li>
<li>non-linear function (neural network)</li>
</ul>
</li>
<li>Many degrees of randomness: This can be made by artificially adding noise to
the dataset.</li>
<li>Vary the number of actions and the number of context features: It would be good
to test algorithms on datasets that have a high and low number of actions, this can
also be said about the dimensionality of the context.</li>
<li>Vary the number of samples: If the context repeats more often, then it
should be easier for the algorithm to learn.</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>[1] Cortes, David. “Adapting Multi-Armed Bandits Policies to Contextual Bandits Scenarios.” ArXiv:1811.04383 [Cs, Stat], Nov. 2019. arXiv.org, <a href="http://arxiv.org/abs/1811.04383">http://arxiv.org/abs/1811.04383</a>.</li>
<li>[2] Bietti, Alberto, et al. “A Contextual Bandit Bake-Off.” ArXiv:1802.04064 [Cs, Stat], Jan. 2020. arXiv.org, <a href="http://arxiv.org/abs/1802.04064">http://arxiv.org/abs/1802.04064</a>.</li>
</ul>
<p>Next Article: <a href="https://renan-cunha.github.io/posts/greedy/"><strong><em>Solving Contextual Bandits with Greediness</em></strong></a>
Previous Article: <a href="https://renan-cunha.github.io/posts/contextual-bandits/"><strong><em>A Very Short Intro to Contextual Bandits</em></strong></a></p>


				</div>
				
				<div class="post">

					<div class="post-heading">
						<h1><a href="https://renan-cunha.github.io/posts/contextual-bandits/">A Very Short Intro to Contextual Bandits</a></h1>
						<span>Feb 24, 2020</span>
					</div>

					<p>In the contextual bandits problem, we have an
environment with <em>K</em> possible actions. The environment returns some context <em>x</em>, taken from a distribution <em>X</em>.
The environment also has a function <em>f(x, k)</em>, that calculates the reward
based on the context and the chosen action <em>k</em>.</p>
<p>The goal then is to find the policy $\pi$ that maximizes the rewards obtained in the
long-term. A policy is a function that maps contexts to actions.</p>
<p>The diagram below illustrates the problem:</p>
<p><img src="https://renan-cunha.github.io/intro.png" alt="contextual-bandits-diagram"></p>
<p>Explaining the steps:</p>
<ul>
<li>The environment displays a context <em>x</em>.</li>
<li>The agent chooses an action <em>k</em> based on the observed context.</li>
<li>The environment returns a reward based on the most recent context and action.</li>
</ul>
<p>This process continues without a determined time-limit.</p>
<p>Additional considerations can be taken into account, like statistical efficiency (if the
algorithm learns fast considering the number of examples) and
computational complexity.</p>
<h2 id="example">Example</h2>
<ul>
<li>Agent: A doctor.</li>
<li>Environment: Every day, a random patient comes into the hospital with a
disease X (the disease is the same to all patients).
<ul>
<li>Context: Features of the patient like age, sex, etc.</li>
<li>Actions: K-number of medicines that the doctor can prescribe.</li>
<li>Reward: 1 if the patient was cured, 0 otherwise.</li>
</ul>
</li>
</ul>
<h2 id="references">References</h2>
<p>Slivkins, Aleksandrs. “Introduction to Multi-Armed Bandits.” ArXiv:1904.07272 [Cs, Stat], Sept. 2019. arXiv.org, <a href="http://arxiv.org/abs/1904.07272">http://arxiv.org/abs/1904.07272</a>.</p>
<h3 id="heading"></h3>
<p>Next Article: <a href="https://renan-cunha.github.io/posts/benchmark/"><strong><em>Brainstorming Benchmarks for Contextual
Bandits</em></strong></a></p>


				</div>
				

				<div class="text-center">
					


				</div>

			</div>
			<div class="col-sm-1 col-md-3 col-md-4">
			</div>
		</div>
	</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-159089871-1', 'auto');
	
	ga('send', 'pageview');
}
</script>





<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

<script src="https://renan-cunha.github.io//js/bootstrap.min.js"></script>

</body>
</html>